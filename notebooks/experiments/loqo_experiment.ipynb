{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Table of Contents\n",
    "* [1 Imports](#chapter1)\n",
    "* [2 Experiment](#chapter2)\n",
    "    * [2.1 Traditional methods](#section_2_1)\n",
    "    * [2.2 XLM-R](#section_2_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports <a class=\"anchor\" id=\"chapter1\"></a>\n",
    "* Import necessary libraries and data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import configs\n",
    "from utils import utils\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from utils.experiment_models import RandomClassifier as RC\n",
    "from utils.experiment_models import RepeatedBERT\n",
    "import ktrain\n",
    "from ktrain import text\n",
    "\n",
    "#data\n",
    "finalized_dataset = pd.read_csv('../../data/arabic_dataset.csv', index_col=0)\n",
    "\n",
    "#config variables\n",
    "RANDOM_SEED = configs.RANDOM_SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment <a class=\"anchor\" id=\"chapter2\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "                         Question  \\\n0   عرف مصطلح الجريمة الإلكترونية   \n1   عرف مصطلح الجريمة الإلكترونية   \n2   عرف مصطلح الجريمة الإلكترونية   \n3   عرف مصطلح الجريمة الإلكترونية   \n4   عرف مصطلح الجريمة الإلكترونية   \n\n                                        Right_Answer  Grade Number  \\\n0   هي كل سلوك غير قانوني يتم باستخدام الأجهزة ال...  3.000    [1]   \n1   هي كل سلوك غير قانوني يتم باستخدام الأجهزة ال...  5.000    [2]   \n2   هي كل سلوك غير قانوني يتم باستخدام الأجهزة ال...  2.625    [3]   \n3   هي كل سلوك غير قانوني يتم باستخدام الأجهزة ال...  4.000    [4]   \n4   هي كل سلوك غير قانوني يتم باستخدام الأجهزة ال...  3.500    [5]   \n\n                                            Response  Question_Nr  Labels  \n0  هي سلوك غير أخلاقي يتم عن طريق وسائل الكترونية...            1       1  \n1  هي كل سلوك غير أخلاقي يتم بواسطة الاجهزة الالك...            1       1  \n2  هي سلوك غير قانوني يحمل باستعمال الأجهزة الالك...            1       1  \n3  هي سلوك غير قانوني تستخدم الوسائل الالكترونية ...            1       1  \n4  هي كل سلوك غير أخلاقي يتم باستخدام الوسائل الا...            1       1  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Right_Answer</th>\n      <th>Grade</th>\n      <th>Number</th>\n      <th>Response</th>\n      <th>Question_Nr</th>\n      <th>Labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>عرف مصطلح الجريمة الإلكترونية</td>\n      <td>هي كل سلوك غير قانوني يتم باستخدام الأجهزة ال...</td>\n      <td>3.000</td>\n      <td>[1]</td>\n      <td>هي سلوك غير أخلاقي يتم عن طريق وسائل الكترونية...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>عرف مصطلح الجريمة الإلكترونية</td>\n      <td>هي كل سلوك غير قانوني يتم باستخدام الأجهزة ال...</td>\n      <td>5.000</td>\n      <td>[2]</td>\n      <td>هي كل سلوك غير أخلاقي يتم بواسطة الاجهزة الالك...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>عرف مصطلح الجريمة الإلكترونية</td>\n      <td>هي كل سلوك غير قانوني يتم باستخدام الأجهزة ال...</td>\n      <td>2.625</td>\n      <td>[3]</td>\n      <td>هي سلوك غير قانوني يحمل باستعمال الأجهزة الالك...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>عرف مصطلح الجريمة الإلكترونية</td>\n      <td>هي كل سلوك غير قانوني يتم باستخدام الأجهزة ال...</td>\n      <td>4.000</td>\n      <td>[4]</td>\n      <td>هي سلوك غير قانوني تستخدم الوسائل الالكترونية ...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>عرف مصطلح الجريمة الإلكترونية</td>\n      <td>هي كل سلوك غير قانوني يتم باستخدام الأجهزة ال...</td>\n      <td>3.500</td>\n      <td>[5]</td>\n      <td>هي كل سلوك غير أخلاقي يتم باستخدام الوسائل الا...</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#renaming some columns to be the same name as other dataset\n",
    "finalized_dataset = finalized_dataset.rename(columns={'Number of Question': 'Question_Nr', 'label': 'Labels', 'Responses':'Response'})\n",
    "finalized_dataset.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#global parameters\n",
    "TEST_SIZE = 0.15\n",
    "VALID_SIZE = 0.15\n",
    "ITERATIONS = 10\n",
    "METRICS = ['average_precision', 'spearman', 'roc_auc', 'averaged_classification_report']\n",
    "AMOUNT_QUESTIONS = len(finalized_dataset.Question_Nr.unique())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Traditional methods  <a class=\"anchor\" id=\"section_2_1\"></a>"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#FOR REPRESENTATION\n",
    "vectorizer = TfidfVectorizer(max_features=50000, ngram_range=(1,1))#TfidfVectorizer(max_features=50000, ngram_range=(1,1))\n",
    "\n",
    "#MODELS\n",
    "#logistic regressor:\n",
    "regressor = LogisticRegression(max_iter=400,random_state=RANDOM_SEED)\n",
    "#random forest:\n",
    "rf = RandomForestClassifier(random_state=RANDOM_SEED)\n",
    "#1-nn:\n",
    "one_NN = KNeighborsClassifier(n_neighbors=1)\n",
    "#3-nn:\n",
    "three_NN = KNeighborsClassifier(n_neighbors=3)\n",
    "#random classifier:\n",
    "rc = RC.RandomClassifier(random_state=RANDOM_SEED, change_state=True)\n",
    "\n",
    "MODELS_MAP = {\n",
    "    'Logistic Regressor':regressor,\n",
    "    'Random Forest':rf,\n",
    "    '1-NN':one_NN,\n",
    "    '3-NN':three_NN,\n",
    "    'Random Classifier':rc\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ljung\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ljung\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ljung\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ljung\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ljung\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ljung\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ljung\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ljung\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ljung\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\ljung\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#LVO EXPERIMENT\n",
    "lvo_map = {} #create lvo map to keep track of predictions,probabilites,scores for each pass\n",
    "for model_label, _ in MODELS_MAP.items():\n",
    "    lvo_map[model_label] = {}\n",
    "\n",
    "#used for concat\n",
    "auc_scores = []\n",
    "auprc_zero_scores = []\n",
    "auprc_one_scores = []\n",
    "spearmans = []\n",
    "class_reports = []\n",
    "\n",
    "#iterate all questions and train/test in a loqo fashion\n",
    "for question_nr in range(0, AMOUNT_QUESTIONS+1, 1):\n",
    "    test_data = finalized_dataset[finalized_dataset.Question_Nr == question_nr]\n",
    "    train_data = finalized_dataset[finalized_dataset.Question_Nr != question_nr]\n",
    "\n",
    "    x_train = train_data.Response\n",
    "    y_train = train_data.Labels\n",
    "    x_test = test_data.Response\n",
    "    y_test = test_data.Labels\n",
    "\n",
    "    if len(y_test.unique()) < 2:\n",
    "        continue #only one class in the test data\n",
    "\n",
    "    vectorizer.fit(x_train)\n",
    "    x_train_processed = vectorizer.transform(x_train)\n",
    "    x_test_processed = vectorizer.transform(x_test)\n",
    "\n",
    "    corr_data = test_data.Grade\n",
    "\n",
    "    for model_label, model in MODELS_MAP.items():\n",
    "        model.fit(x_train_processed, y_train)\n",
    "        predictions = model.predict(x_test_processed)\n",
    "        predict_probas = model.predict_proba(x_test_processed)\n",
    "\n",
    "        positive_probas = np.array(predict_probas)[:,1]\n",
    "        negative_probas = np.array(predict_probas)[:,0]\n",
    "\n",
    "        auc = roc_auc_score(y_test, positive_probas)\n",
    "        auprc_one = average_precision_score(y_test, positive_probas, pos_label=1)\n",
    "        auprc_zero = average_precision_score(y_test, negative_probas, pos_label=0)\n",
    "        spearman_corr, _ = spearmanr(positive_probas, corr_data)\n",
    "        class_report = classification_report(y_test, predictions, output_dict=True)\n",
    "\n",
    "        auc_scores.append(auc)\n",
    "        auprc_one_scores.append(auprc_one)\n",
    "        auprc_zero_scores.append(auprc_zero)\n",
    "        spearmans.append(spearman_corr)\n",
    "        class_reports.append(class_report)\n",
    "\n",
    "        lvo_results = lvo_map.get(model_label)\n",
    "        lvo_results['true_labels'] = lvo_results.get('true_labels', []) + list(y_test)\n",
    "        lvo_results['predicted_labels'] =  lvo_results.get('predicted_labels', []) + list(predictions)\n",
    "        lvo_results['predicted_probas'] = lvo_results.get('predicted_probas', []) + list(predict_probas)\n",
    "        lvo_results['corr_data'] = lvo_results.get('corr_data',[]) + list(corr_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro (concat):\n",
      "Model: Logistic Regressor. AUC: 0.49. AUPRC (Pos class: 0): 0.33. AUPRC (Pos class: 1): 0.66. Spearman -0.00. \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.13      0.18       723\n",
      "           1       0.64      0.82      0.72      1361\n",
      "\n",
      "    accuracy                           0.58      2084\n",
      "   macro avg       0.46      0.48      0.45      2084\n",
      "weighted avg       0.51      0.58      0.53      2084\n",
      "\n",
      "\n",
      "Macro (Average):\n",
      "Model: Logistic Regressor. Average AUC: 0.50. Average AUPRC (Pos class: 0): 0.39. Average AUPRC (Pos class: 1): 0.67. Average Spearman 0.02. \n",
      "\n",
      "0-> precision: 0.35, recall: 0.35, f1-score: 0.29, support: 15.38, \n",
      "1-> precision: 0.63, recall: 0.64, f1-score: 0.60, support: 28.96, \n",
      "accuracy-> 0.52\n",
      "macro avg-> precision: 0.49, recall: 0.49, f1-score: 0.44, support: 44.34, \n",
      "weighted avg-> precision: 0.60, recall: 0.52, f1-score: 0.52, support: 44.34, \n",
      "\n",
      "***************************************************************\n",
      "Micro (concat):\n",
      "Model: Random Forest. AUC: 0.50. AUPRC (Pos class: 0): 0.34. AUPRC (Pos class: 1): 0.66. Spearman 0.03. \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.26      0.30       723\n",
      "           1       0.65      0.75      0.70      1361\n",
      "\n",
      "    accuracy                           0.58      2084\n",
      "   macro avg       0.50      0.50      0.50      2084\n",
      "weighted avg       0.55      0.58      0.56      2084\n",
      "\n",
      "\n",
      "Macro (Average):\n",
      "Model: Random Forest. Average AUC: 0.50. Average AUPRC (Pos class: 0): 0.39. Average AUPRC (Pos class: 1): 0.67. Average Spearman 0.02. \n",
      "\n",
      "0-> precision: 0.35, recall: 0.35, f1-score: 0.29, support: 15.38, \n",
      "1-> precision: 0.63, recall: 0.64, f1-score: 0.60, support: 28.96, \n",
      "accuracy-> 0.52\n",
      "macro avg-> precision: 0.49, recall: 0.49, f1-score: 0.44, support: 44.34, \n",
      "weighted avg-> precision: 0.60, recall: 0.52, f1-score: 0.52, support: 44.34, \n",
      "\n",
      "***************************************************************\n",
      "Micro (concat):\n",
      "Model: 1-NN. AUC: 0.50. AUPRC (Pos class: 0): 0.35. AUPRC (Pos class: 1): 0.65. Spearman 0.01. \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.46      0.39       723\n",
      "           1       0.65      0.55      0.60      1361\n",
      "\n",
      "    accuracy                           0.52      2084\n",
      "   macro avg       0.50      0.50      0.50      2084\n",
      "weighted avg       0.55      0.52      0.53      2084\n",
      "\n",
      "\n",
      "Macro (Average):\n",
      "Model: 1-NN. Average AUC: 0.50. Average AUPRC (Pos class: 0): 0.39. Average AUPRC (Pos class: 1): 0.67. Average Spearman 0.02. \n",
      "\n",
      "0-> precision: 0.35, recall: 0.35, f1-score: 0.29, support: 15.38, \n",
      "1-> precision: 0.63, recall: 0.64, f1-score: 0.60, support: 28.96, \n",
      "accuracy-> 0.52\n",
      "macro avg-> precision: 0.49, recall: 0.49, f1-score: 0.44, support: 44.34, \n",
      "weighted avg-> precision: 0.60, recall: 0.52, f1-score: 0.52, support: 44.34, \n",
      "\n",
      "***************************************************************\n",
      "Micro (concat):\n",
      "Model: 3-NN. AUC: 0.47. AUPRC (Pos class: 0): 0.33. AUPRC (Pos class: 1): 0.64. Spearman -0.03. \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.36      0.33       723\n",
      "           1       0.63      0.58      0.60      1361\n",
      "\n",
      "    accuracy                           0.50      2084\n",
      "   macro avg       0.47      0.47      0.47      2084\n",
      "weighted avg       0.52      0.50      0.51      2084\n",
      "\n",
      "\n",
      "Macro (Average):\n",
      "Model: 3-NN. Average AUC: 0.50. Average AUPRC (Pos class: 0): 0.39. Average AUPRC (Pos class: 1): 0.67. Average Spearman 0.02. \n",
      "\n",
      "0-> precision: 0.35, recall: 0.35, f1-score: 0.29, support: 15.38, \n",
      "1-> precision: 0.63, recall: 0.64, f1-score: 0.60, support: 28.96, \n",
      "accuracy-> 0.52\n",
      "macro avg-> precision: 0.49, recall: 0.49, f1-score: 0.44, support: 44.34, \n",
      "weighted avg-> precision: 0.60, recall: 0.52, f1-score: 0.52, support: 44.34, \n",
      "\n",
      "***************************************************************\n",
      "Micro (concat):\n",
      "Model: Random Classifier. AUC: 0.50. AUPRC (Pos class: 0): 0.35. AUPRC (Pos class: 1): 0.65. Spearman -0.01. \n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.49      0.41       723\n",
      "           1       0.66      0.52      0.58      1361\n",
      "\n",
      "    accuracy                           0.51      2084\n",
      "   macro avg       0.51      0.51      0.50      2084\n",
      "weighted avg       0.55      0.51      0.52      2084\n",
      "\n",
      "\n",
      "Macro (Average):\n",
      "Model: Random Classifier. Average AUC: 0.50. Average AUPRC (Pos class: 0): 0.39. Average AUPRC (Pos class: 1): 0.67. Average Spearman 0.02. \n",
      "\n",
      "0-> precision: 0.35, recall: 0.35, f1-score: 0.29, support: 15.38, \n",
      "1-> precision: 0.63, recall: 0.64, f1-score: 0.60, support: 28.96, \n",
      "accuracy-> 0.52\n",
      "macro avg-> precision: 0.49, recall: 0.49, f1-score: 0.44, support: 44.34, \n",
      "weighted avg-> precision: 0.60, recall: 0.52, f1-score: 0.52, support: 44.34, \n",
      "\n",
      "***************************************************************\n"
     ]
    }
   ],
   "source": [
    "#PRINT RESULTS\n",
    "#Micro is what is presented in the paper. Macro is an average per pass.\n",
    "corr_data = finalized_dataset.Grade\n",
    "for model, results in lvo_map.items():\n",
    "    true_labels = results['true_labels']\n",
    "    predicted_labels = results['predicted_labels']\n",
    "    predicted_probas = results['predicted_probas']\n",
    "    corr_data = results['corr_data']\n",
    "    positive_probas = np.array(predicted_probas)[:,1]\n",
    "    negative_probas = np.array(predicted_probas)[:,0]\n",
    "\n",
    "    auc = roc_auc_score(true_labels, positive_probas)\n",
    "    auprc_one = average_precision_score(true_labels, positive_probas, pos_label=1)\n",
    "    auprc_zero = average_precision_score(true_labels, negative_probas, pos_label=0)\n",
    "    spearman_corr, _ = spearmanr(positive_probas, corr_data)\n",
    "    class_report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "    print('Micro (concat):')\n",
    "    print('Model: {}. AUC: {:0.2f}. AUPRC (Pos class: 0): {:0.2f}. AUPRC (Pos class: 1): {:0.2f}. Spearman {:0.2f}. \\n'.\n",
    "        format(model, auc, auprc_zero, auprc_one, spearman_corr))\n",
    "    print(class_report + '\\n')\n",
    "\n",
    "\n",
    "    averaged_report = utils.average_report(class_reports)\n",
    "    print('Macro (Average):')\n",
    "    print('Model: {}. Average AUC: {:0.2f}. Average AUPRC (Pos class: 0): {:0.2f}. Average AUPRC (Pos class: 1): {:0.2f}. Average Spearman {:0.2f}. \\n'.\n",
    "        format(model, mean(auc_scores), mean(auprc_zero_scores), mean(auprc_one_scores), mean(spearmans)))\n",
    "    print(utils.format_report(averaged_report))\n",
    "\n",
    "    print('***************************************************************')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 XLM-R <a class=\"anchor\" id=\"section_2_2\"></a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#xlm-r parameters\n",
    "MODEL_NAME = \"xlm-roberta-base\"\n",
    "BATCH_SIZE = 6\n",
    "MAX_LEN = 200\n",
    "CLASS_NAMES = [0,1]\n",
    "EPOCHS = 15\n",
    "LR = 1e-5\n",
    "#xlm-r\n",
    "t = text.Transformer(MODEL_NAME, maxlen=200, class_names=CLASS_NAMES)\n",
    "#Create RepeatedBert object for experiments. (See RepeatedBERT.py in utils>experiment_models>RepeatedBERT)\n",
    "repeated_bert = RepeatedBERT.RepeatedBERT(transformer=t,metrics=METRICS,iterations=ITERATIONS,random_state=RANDOM_SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "auc_scores = []\n",
    "auprc_zero_scores = []\n",
    "auprc_one_scores = []\n",
    "spearmans = []\n",
    "class_reports = []\n",
    "lvo_results = {}\n",
    "\n",
    "for question_nr in range(0, AMOUNT_QUESTIONS+1, 1):\n",
    "    test_data = finalized_dataset[finalized_dataset.Question_Nr == question_nr]\n",
    "    train_data = finalized_dataset[finalized_dataset.Question_Nr != question_nr]\n",
    "\n",
    "    x_train = train_data.Response\n",
    "    y_train = train_data.Labels\n",
    "    x_test = test_data.Response\n",
    "    y_test = test_data.Labels\n",
    "\n",
    "    x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=TEST_SIZE,stratify=y_train,random_state=RANDOM_SEED)\n",
    "\n",
    "    if len(y_test.unique()) < 2:\n",
    "        continue #only one class in the test data\n",
    "\n",
    "    corr_data = test_data.Grade\n",
    "\n",
    "    trn = t.preprocess_train(list(x_train), list(y_train))\n",
    "    val = t.preprocess_test(list(x_dev), list(y_dev))\n",
    "    model = t.get_classifier()\n",
    "    learner = ktrain.get_learner(model, train_data=trn, val_data=val, batch_size=BATCH_SIZE)\n",
    "    learner.fit_onecycle(LR,EPOCHS)\n",
    "    learner.validate(class_names=t.get_classes())\n",
    "    predictor = ktrain.get_predictor(learner.model, preproc=t)\n",
    "\n",
    "    predictions = predictor.predict(list(x_test))\n",
    "    predict_probas = predictor.predict_proba(list(x_test))\n",
    "\n",
    "    pos_probas = np.array(predict_probas)[:,1]\n",
    "    neg_probas = np.array(predict_probas)[:,0]\n",
    "\n",
    "    auc = roc_auc_score(y_test, pos_probas)\n",
    "    auprc_one = average_precision_score(y_test, pos_probas, pos_label=1)\n",
    "    auprc_zero = average_precision_score(y_test, neg_probas, pos_label=0)\n",
    "    spearman_corr, _ = spearmanr(pos_probas, corr_data)\n",
    "    class_report = classification_report(y_test, predictions, output_dict=True)\n",
    "\n",
    "    auc_scores.append(auc)\n",
    "    auprc_one_scores.append(auprc_one)\n",
    "    auprc_zero_scores.append(auprc_zero)\n",
    "    spearmans.append(spearman_corr)\n",
    "    class_reports.append(class_report)\n",
    "\n",
    "    lvo_results['true_labels'] = lvo_results.get('true_labels',[]) + list(y_test)\n",
    "    lvo_results['predicted_labels'] = lvo_results.get('predicted_labels',[]) + list(predictions)\n",
    "    lvo_results['predicted_probas'] = lvo_results.get('predicted_probas',[]) + list(predict_probas)\n",
    "    lvo_results['corr_data'] = lvo_results.get('corr_data',[]) + list(corr_data)\n",
    "\n",
    "    print('q : ', question_nr)\n",
    "    print('y_true: ', y_test)\n",
    "    print('predictions: ', predictions)\n",
    "    print('predict probas: ', predict_probas)\n",
    "    print('corr data: ', corr_data)\n",
    "\n",
    "    del trn, val, model, learner, predictor # delete for each pass to reduce unec. disk space"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#PRINT RESULTS\n",
    "true_labels = lvo_results['true_labels']\n",
    "predicted_labels = lvo_results['predicted_labels']\n",
    "predicted_probas = lvo_results['predicted_probas']\n",
    "corr_data = lvo_results['corr_data']\n",
    "positive_probas = np.array(predicted_probas)[:,1]\n",
    "negative_probas = np.array(predicted_probas)[:,0]\n",
    "\n",
    "auc = roc_auc_score(true_labels, positive_probas)\n",
    "auprc_one = average_precision_score(true_labels, positive_probas, pos_label=1)\n",
    "auprc_zero = average_precision_score(true_labels, negative_probas, pos_label=0)\n",
    "spearman_corr, _ = spearmanr(positive_probas, corr_data)\n",
    "class_report = classification_report(true_labels, predicted_labels)\n",
    "\n",
    "print('Micro (concat):')\n",
    "print('Model: XLM-R. AUC: {:0.2f}. AUPRC (Pos class: 0): {:0.2f}. AUPRC (Pos class: 1): {:0.2f}. Spearman {:0.2f}. \\n'.\n",
    "    format(auc, auprc_zero, auprc_one, spearman_corr))\n",
    "print(class_report + '\\n')\n",
    "\n",
    "\n",
    "averaged_report = utils.average_report(class_reports)\n",
    "print('Macro (Average):')\n",
    "print('Model: XLM-R. Average AUC: {:0.2f}. Average AUPRC (Pos class: 0): {:0.2f}. Average AUPRC (Pos class: 1): {:0.2f}. Average Spearman {:0.2f}. \\n'.\n",
    "     format(mean(auc_scores), mean(auprc_zero_scores), mean(auprc_one_scores), mean(spearmans)))\n",
    "print(utils.format_report(averaged_report))\n",
    "\n",
    "print('***************************************************************')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Again, Micro is what is presented in the paper. Code ran on COLAB, these were the results:\n",
    "# Micro (concat):\n",
    "# Model: XLM-R. AUC: 0.56. AUPRC (Pos class: 0): 0.39. AUPRC (Pos class: 1): 0.70. Spearman 0.17.\n",
    "#\n",
    "#               precision    recall  f1-score   support\n",
    "#\n",
    "#            0       0.40      0.44      0.42       723\n",
    "#            1       0.69      0.65      0.67      1361\n",
    "#\n",
    "#     accuracy                           0.58      2084\n",
    "#    macro avg       0.54      0.54      0.54      2084\n",
    "# weighted avg       0.59      0.58      0.58      2084\n",
    "#\n",
    "#\n",
    "# Macro (Average):\n",
    "# Model: XLM-R. Average AUC: 0.57. Average AUPRC (Pos class: 0): 0.50. Average AUPRC (Pos class: 1): 0.72. Average Spearman 0.21.\n",
    "#\n",
    "# 0-> precision: 0.45, recall: 0.46, f1-score: 0.38, support: 15.38,\n",
    "# 1-> precision: 0.65, recall: 0.65, f1-score: 0.62, support: 28.96,\n",
    "# accuracy-> 0.56\n",
    "# macro avg-> precision: 0.55, recall: 0.56, f1-score: 0.50, support: 44.34,\n",
    "# weighted avg-> precision: 0.65, recall: 0.56, f1-score: 0.57, support: 44.34,\n",
    "#\n",
    "# ***************************************************************"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}